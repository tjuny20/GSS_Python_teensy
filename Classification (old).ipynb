{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import torch\n",
    "from numpy.array_api import int32\n",
    "from torch.linalg import inv, eig, pinv\n",
    "from matplotlib import pyplot as plt\n",
    "from tools import whiten, adap_whitening, adap_whitening_2\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import pickle\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tools import load, split, estimate_derivative, plot_two_intervals"
   ],
   "id": "623d9c50081bf6bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import font_manager as fm, rcParams\n",
    "\n",
    "path = r\"/home/p308270/.local/share/fonts/Helvetica.ttf\"  # or .otf\n",
    "fm.fontManager.addfont(path)\n",
    "rcParams[\"font.family\"] = fm.FontProperties(fname=path).get_name()"
   ],
   "id": "707290d578d4f5db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_blocks(labels, max_len=20, ignore=0):\n",
    "    \"\"\"\n",
    "    Split a 1D array of integer labels into contiguous blocks.\n",
    "    - Blocks with label == `ignore` (default 0) are skipped.\n",
    "    - Long runs are split into chunks of at most `max_len`.\n",
    "    Returns: list of (start_idx, end_idx, label) with end_idx exclusive.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    prev = labels[0]\n",
    "\n",
    "    # walk + flush on change (and once at the end)\n",
    "    for i in range(1, n + 1):\n",
    "        cur = labels[i] if i < n else None\n",
    "        if cur != prev:\n",
    "            if prev != ignore:\n",
    "                run_start, run_end, lab = start, i, int(prev)\n",
    "                # chunk the run to respect max_len\n",
    "                s = run_start\n",
    "                while s < run_end:\n",
    "                    e = min(s + max_len, run_end)\n",
    "                    blocks.append((s, e, lab))\n",
    "                    s = e\n",
    "            start = i\n",
    "            prev = cur\n",
    "\n",
    "    return blocks"
   ],
   "id": "59fe18e2648b4a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_hd = 10000\n",
    "n_out = 3\n",
    "k = 50\n",
    "n_pot = 10\n",
    "n_train = 225\n",
    "w_teacher = 1.\n",
    "t_training_delay = 5.\n",
    "filename = '1_600_20'\n",
    "\n",
    "grid_uniformW = [False]\n",
    "grid_normalized = [False]\n",
    "grid_whitened = [False]\n",
    "grid_k = [10, 15, 20, 25]\n",
    "grid_n_pot = [n for n in range(2, 20, 2)]\n",
    "grid_t_training_delay = [n for n in range(0, 20, 5)]\n",
    "grid_n_fold = 5\n",
    "\n",
    "\n",
    "sensor_data, sequence, times_sec, sequence_sec = load(filename, reduced=True)\n",
    "d_sensor_data = np.apply_along_axis(estimate_derivative, axis=0, arr=sensor_data)\n",
    "sensor_data = np.hstack((sensor_data, d_sensor_data))\n",
    "\n",
    "# baseline = np.mean(sensor_data[:300], axis=0)  # Add baseline substraction\n",
    "# sensor_data = (sensor_data - baseline)"
   ],
   "id": "2f0b0b8937a31a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "params = {'uniformW': [], 'normalized': [], 'whitened': [], 'k': [], 'n_pot': [], 't_training_delay': [], 'n_fold': []}\nresults = {'train_acc': [], 'test_acc': []}\nfor uniformW in grid_uniformW:\n    for normalized in grid_normalized:\n        for whitened in grid_whitened:\n            for k in grid_k:\n                for n_pot in grid_n_pot:\n                    for t_training_delay in grid_t_training_delay:\n                        for n_fold in range(grid_n_fold):\n                            for key in params.keys():\n                                params[key].append(locals()[key])\n\n                            if normalized:\n                                sensor_data_norm = (sensor_data - np.mean(sensor_data, axis=0))/ np.std(sensor_data, axis=0)\n                            else:\n                                sensor_data_norm = sensor_data\n\n                            # x_dense, _, _, _ = adap_whitening_2(sensor_data_norm)\n                            if whitened:\n                                x_dense = adap_whitening_2(sensor_data_norm)\n                            else:\n                                x_dense = sensor_data_norm\n\n                            n_dense = x_dense.shape[1]\n\n                            labels = np.zeros_like(times_sec)\n                            for i, t in enumerate(sequence_sec[:n_train]):\n                                try:\n                                    flag = (times_sec > sequence_sec[i] + t_training_delay) & (times_sec < sequence_sec[i+1])\n                                except IndexError:\n                                    flag = (times_sec > sequence_sec[i] + t_training_delay)\n                                labels[flag] = int(sequence[i][1])\n\n                            idx_last_flag = np.where(labels != 0)[0][-1]\n\n                            if uniformW:\n                                W_hd = np.random.uniform(high=1/np.sqrt(n_dense), size=(n_hd, n_dense))  #Test random sparse weights\n                            else:\n                                W_hd = np.random.binomial(n=1, p=0.05, size=(n_hd, n_dense))  #Test random sparse weights\n                            x_hd = x_dense @ W_hd.T\n                            ranks = np.argsort(np.argsort(-x_hd, axis=1), axis=1)\n                            z_hd = np.where(ranks < k, 1., 0.)\n                            W_out = np.zeros((n_out, n_hd))\n                            W = np.zeros((n_out, n_hd))\n\n                            z_out_train = np.zeros((z_hd.shape[0],  n_out))\n                            for i, row in enumerate(z_hd[:idx_last_flag]):\n                                teacher = np.zeros((n_out,))\n                                if labels[i] != 0:\n                                    teacher[int(labels[i]-1)] = w_teacher\n                                out = row @ W_out.T + teacher\n                                z_out_train[i] = out\n                                dW = (1./n_pot)*(np.atleast_2d(out).T @ np.atleast_2d(row))\n                                W += dW\n                                W_out = np.where(W>=1., 1./k, 0.)\n                                # if i%100 == 0:\n                                #     print(np.sum(W_out, axis=1))\n\n                            z_out = np.zeros((z_hd.shape[0],  n_out))\n                            for i, row in enumerate(z_hd):\n                                out = row @ W_out.T\n                                z_out[i] = out\n\n                            ranks_out = np.argsort(np.argsort(-z_out, axis=1), axis=1)\n                            z_wta = np.where(ranks_out < 1, 1., 0.)\n\n                            z_pred = np.zeros_like(sequence_sec)\n                            z_true = np.zeros_like(sequence_sec)\n                            for i, t in enumerate(sequence_sec):\n                                try:\n                                    flag = (times_sec > sequence_sec[i] + t_training_delay) & (times_sec < sequence_sec[i+1])\n                                except IndexError:\n                                    flag = (times_sec > sequence_sec[i] + t_training_delay)\n                                z_pred[i] = np.argsort(np.sum(z_out[flag], axis=0))[-1] + 1\n                                z_true[i] = sequence[i][1]\n\n                            train_acc = sklearn.metrics.accuracy_score(z_true[:n_train], z_pred[:n_train])\n                            test_acc = sklearn.metrics.accuracy_score(z_true[n_train:], z_pred[n_train:])\n                            results['train_acc'].append(train_acc)\n                            results['test_acc'].append(test_acc)\n                            results['y_pred'].append(z_pred.copy())\n                            results['y_true'].append(z_true.copy())\n\n                            print(f'UniformW: {uniformW}, Normalized: {normalized}, Whitened: {whitened}, k: {k}, n_pot: {n_pot}, t_training_delay: {t_training_delay}, n_fold: {n_fold}')\n                            print(f'Train accuracy: {train_acc:.4f}, Test accuracy: {test_acc:.4f}')\n\nfor key in params.keys():\n    params[key] = np.array(params[key])\nfor k in ['train_acc', 'test_acc']:\n    results[k] = np.array(results[k])\n# keep y_pred / y_true as lists-of-arrays (ragged OK) or cast to object arrays:\nresults['y_pred'] = np.array(results['y_pred'], dtype=object)\nresults['y_true'] = np.array(results['y_true'], dtype=object)\n\ndata = {'params': params, 'results': results}\n\nwith open('data/gridsearch_full.pkl', 'wb') as f:\n    pickle.dump(data, f)",
   "id": "41852f9821ff1516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grid_k = [10, 15, 20, 25]\n",
    "grid_n_pot = [n for n in range(2, 20, 2)]\n",
    "grid_t_training_delay = [n for n in range(0, 20, 5)]\n",
    "grid_n_fold = 5\n",
    "\n",
    "with open('data/gridsearch_full.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "params = data['params']\n",
    "results = data['results']\n",
    "\n",
    "for key in params.keys():\n",
    "    params[key] = np.array(params[key])\n",
    "for key in results.keys():\n",
    "    results[key] = np.array(results[key])\n",
    "\n",
    "ims = np.zeros((len(grid_k), len(grid_n_pot), len(grid_t_training_delay)))  # mean test acc\n",
    "stds = np.zeros_like(ims)   # std of test acc\n",
    "maxs = np.zeros_like(ims)   # max single-fold test acc\n",
    "\n",
    "for i, k in enumerate(grid_k):\n",
    "    for j, n_pot in enumerate(grid_n_pot):\n",
    "        for l, t_training_delay in enumerate(grid_t_training_delay):\n",
    "\n",
    "            flags = np.stack((params['k']==k,\n",
    "                              params['n_pot']==n_pot,\n",
    "                              params['t_training_delay']==t_training_delay), axis=1).all(axis=1)\n",
    "\n",
    "            # collect values for this combo\n",
    "            vals_train = np.asarray(results['train_acc'][flags], dtype=float)\n",
    "            vals_test  = np.asarray(results['test_acc'][flags],  dtype=float)\n",
    "\n",
    "            # store summary stats for plotting/selection\n",
    "            ims[i, j, l]  = np.nanmean(vals_test)\n",
    "            stds[i, j, l] = np.nanstd(vals_test)\n",
    "            maxs[i, j, l] = np.nanmax(vals_test)\n",
    "\n",
    "# === NEW: print best combo and its stats ===\n",
    "best_idx = np.unravel_index(np.nanargmax(ims), ims.shape)\n",
    "best_k   = grid_k[best_idx[0]]\n",
    "best_np  = grid_n_pot[best_idx[1]]\n",
    "best_td  = grid_t_training_delay[best_idx[2]]\n",
    "\n",
    "best_flags = np.stack((params['k']==best_k,\n",
    "                       params['n_pot']==best_np,\n",
    "                       params['t_training_delay']==best_td), axis=1).all(axis=1)\n",
    "best_vals  = np.asarray(results['test_acc'][best_flags], dtype=float)\n",
    "# NEW: identify which n_fold gave the best single-fold accuracy within this best combo\n",
    "best_folds  = np.asarray(params['n_fold'][best_flags], dtype=int)\n",
    "best_fold_i = int(np.nanargmax(best_vals))\n",
    "best_n_fold = int(best_folds[best_fold_i])\n",
    "\n",
    "print(\n",
    "    f\"Best params → k={best_k}, N_pot={best_np}, t_delay={best_td} s\\n\"\n",
    "    f\"Highest mean test accuracy: {np.nanmean(best_vals):.4f} ± {np.nanstd(best_vals):.4f} (n={best_vals.size})\\n\"\n",
    "    f\"Highest single-fold test accuracy at those params: {np.nanmax(best_vals):.4f}, n_fold={best_n_fold}\"\n",
    ")\n",
    "\n",
    "# ===== plotting stays the same =====\n",
    "fig, ax = plt.subplots(1, 5, figsize=(10, 5))\n",
    "for i, k in enumerate(grid_k):\n",
    "    im = ax[i].imshow(ims[i],\n",
    "                      cmap='viridis', aspect='equal', origin='lower', vmin=0, vmax=1)\n",
    "    ax[i].set_xticks(np.arange(len(grid_t_training_delay)), labels=grid_t_training_delay)\n",
    "    ax[i].set_yticks(np.arange(len(grid_n_pot)), labels=grid_n_pot)\n",
    "    ax[i].tick_params(left=False, bottom=False)\n",
    "    ax[i].set_title(f'$k={k}$')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax[4], fraction=1, shrink=0.5)\n",
    "cbar.set_ticks([0,1])\n",
    "cbar.set_label('Test accuracy')\n",
    "ax[-1].set_visible(False)\n",
    "for a in ax[1:4]:\n",
    "    a.set_yticks([])\n",
    "for a in ax[:4]:\n",
    "    a.set_xlabel('$t_{delay}$ (s)')\n",
    "    a.spines[:].set_visible(False)\n",
    "ax[0].set_ylabel('$N_{pot}$')\n",
    "\n",
    "for l in range(4):\n",
    "    for (j,i),label in np.ndenumerate(ims[l]):\n",
    "        ax[l].text(i,j,f'{label:.2}',ha='center',va='center', fontsize=10)\n",
    "\n",
    "plt.savefig('figs/gridsearch_raw.png', dpi=500)\n",
    "plt.savefig('figs/gridsearch_raw.pdf', dpi=500)\n",
    "plt.show()\n"
   ],
   "id": "671c55974539d86a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def find_config_index(params, target):\n",
    "    \"\"\"\n",
    "    params: dict of arrays like in your file (uniformW, normalized, whitened, k, n_pot, t_training_delay, n_fold)\n",
    "    target: dict with the exact values to match, e.g.\n",
    "            dict(uniformW=True, normalized=True, whitened=False, k=256, n_pot=50, t_training_delay=0.0, n_fold=0)\n",
    "    Returns the first matching index (int). Raises if none found.\n",
    "    \"\"\"\n",
    "    keys = list(target.keys())\n",
    "    mask = np.ones_like(params[keys[0]], dtype=bool)\n",
    "    for k in keys:\n",
    "        v = target[k]\n",
    "        # be tolerant with float comparison\n",
    "        if isinstance(v, float):\n",
    "            mask &= np.isclose(params[k], v)\n",
    "        else:\n",
    "            mask &= (params[k] == v)\n",
    "    idxs = np.flatnonzero(mask)\n",
    "    if len(idxs) == 0:\n",
    "        raise ValueError(f\"No configuration matched: {target}\")\n",
    "    if len(idxs) > 1:\n",
    "        print(f\"Warning: {len(idxs)} configs matched; using the first (idx={idxs[0]}).\")\n",
    "    return int(idxs[0])\n",
    "\n",
    "def get_preds_for_index(results, idx):\n",
    "    \"\"\"\n",
    "    Supports two layouts:\n",
    "      - Preferred: results['y_pred'][i], results['y_true'][i] for each config i\n",
    "      - Fallback (your current code): single arrays results['y_pred'], results['y_true']\n",
    "        (these correspond only to the *last* run)\n",
    "    \"\"\"\n",
    "    if 'y_pred' in results and 'y_true' in results:\n",
    "        y_pred = np.asarray(results['y_pred'], dtype=object)\n",
    "        y_true = np.asarray(results['y_true'], dtype=object)\n",
    "        y_pred = np.asarray(y_pred[idx]).astype(int)\n",
    "        y_true = np.asarray(y_true[idx]).astype(int)\n",
    "    else:\n",
    "        # Fallback: use the only arrays available (likely from the last config)\n",
    "        print(\"⚠️ Using results['y_pred'] / results['y_true'] (likely last run only).\")\n",
    "        y_pred = np.asarray(results['y_pred']).astype(int)\n",
    "        y_true = np.asarray(results['y_true']).astype(int)\n",
    "    return y_true, y_pred\n",
    "\n",
    "def plot_confmat_pair(y_true_seq, y_pred_seq, n_train, class_labels=None,\n",
    "                      normalize='true', title_suffix='', saveprefix=None):\n",
    "    \"\"\"\n",
    "    Splits by n_train over the *sequence index* dimension and plots train/test CMs in separate figures.\n",
    "    class_labels: array-like order of labels to show; if None, inferred from data.\n",
    "    normalize: None|'true'|'pred'|'all' (sklearn semantics)\n",
    "    saveprefix: if given, writes '<saveprefix>_train.pdf' and '<saveprefix>_test.pdf'\n",
    "    \"\"\"\n",
    "    y_true_tr = y_true_seq[:n_train]\n",
    "    y_pred_tr = y_pred_seq[:n_train]\n",
    "    y_true_te = y_true_seq[n_train:]\n",
    "    y_pred_te = y_pred_seq[n_train:]\n",
    "\n",
    "    if class_labels is None:\n",
    "        class_labels = np.unique(np.concatenate([y_true_tr, y_pred_tr, y_true_te, y_pred_te]))\n",
    "        class_labels = class_labels[class_labels != 0]  # drop any zeros if present\n",
    "\n",
    "    # Train\n",
    "    cm_tr = confusion_matrix(y_true_tr, y_pred_tr, labels=class_labels,\n",
    "                             normalize=(normalize if normalize else None))\n",
    "    fig_tr, ax_tr = plt.subplots(figsize=(6, 5))\n",
    "    ConfusionMatrixDisplay(cm_tr, display_labels=class_labels).plot(\n",
    "        ax=ax_tr, values_format=('.2f' if normalize else 'd'), colorbar=False)\n",
    "    ax_tr.set_title(f\"Train confusion matrix {title_suffix}\".strip())\n",
    "    ax_tr.set_xlabel(\"Predicted\")\n",
    "    ax_tr.set_ylabel(\"True\")\n",
    "    fig_tr.tight_layout()\n",
    "    if saveprefix:\n",
    "        fig_tr.savefig(f\"{saveprefix}_train.pdf\", bbox_inches='tight')\n",
    "        fig_tr.savefig(f\"{saveprefix}_train.png\",dpi=200, bbox_inches='tight')\n",
    "\n",
    "    # Test\n",
    "    cm_te = confusion_matrix(y_true_te, y_pred_te, labels=class_labels,\n",
    "                             normalize=(normalize if normalize else None))\n",
    "    fig_te, ax_te = plt.subplots(figsize=(6, 5))\n",
    "    ConfusionMatrixDisplay(cm_te, display_labels=class_labels).plot(\n",
    "        ax=ax_te, values_format=('.2f' if normalize else 'd'), colorbar=True)\n",
    "    ax_te.set_title(f\"Test confusion matrix {title_suffix}\".strip())\n",
    "    ax_te.set_xlabel(\"Predicted\")\n",
    "    ax_te.set_ylabel(\"True\")\n",
    "    fig_te.tight_layout()\n",
    "    if saveprefix:\n",
    "        fig_te.savefig(f\"{saveprefix}_test.pdf\", bbox_inches='tight')\n",
    "        fig_te.savefig(f\"{saveprefix}_test.png\", dpi=200, bbox_inches='tight')\n",
    "\n",
    "\n",
    "    return (fig_tr, ax_tr), (fig_te, ax_te)\n",
    "\n",
    "# ---------- usage ----------\n",
    "\n",
    "# Load your saved gridsearch\n",
    "with open('data/gridsearch_full.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "params = data['params']\n",
    "results = data['results']\n",
    "\n",
    "# Choose the configuration you want to visualize:\n",
    "target_cfg = dict(\n",
    "    uniformW=False,\n",
    "    normalized=False,\n",
    "    whitened=False,\n",
    "    k=10,\n",
    "    n_pot=6.,\n",
    "    t_training_delay=5.0,  # float compare is tolerant via isclose\n",
    "    n_fold=0,\n",
    ")\n",
    "\n",
    "idx = find_config_index(params, target_cfg)\n",
    "\n",
    "# Pull predictions for that config (see note below if this warns)\n",
    "y_true_seq, y_pred_seq = get_preds_for_index(results, idx)\n",
    "\n",
    "# If you know the number of training sequences and class IDs, set them here:\n",
    "# n_train must match your experiment; classes default to inferred if None.\n",
    "n_train = 225  # <- use your actual n_train variable from the experiment scope\n",
    "# Optionally, enforce a fixed class order (e.g., 1..n_out):\n",
    "# class_labels = np.arange(1, n_out + 1)\n",
    "class_labels = None\n",
    "\n",
    "# Plot and save PDFs\n",
    "plot_confmat_pair(\n",
    "    y_true_seq=y_true_seq,\n",
    "    y_pred_seq=y_pred_seq,\n",
    "    n_train=n_train,\n",
    "    class_labels=class_labels,\n",
    "    normalize='true',\n",
    "    title_suffix=f\"(k={target_cfg['k']}, fold={target_cfg['n_fold']})\",\n",
    "    saveprefix='figs/confmat'  # -> saves figs/confmat_train.pdf and figs/confmat_test.pdf\n",
    ")\n"
   ],
   "id": "ce8cbdb82e40dd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n_hd = 10000\nn_out = 3\nk = 25\nn_pot = 16\nn_train = 225\nw_teacher = 1.\nt_training_delay = 0.\n\nx_dense = sensor_data\nn_dense = x_dense.shape[1]\n\nlabels = np.zeros_like(times_sec)\nfor i, t in enumerate(sequence_sec[:n_train]):\n    try:\n        flag = (times_sec > sequence_sec[i] + t_training_delay) & (times_sec < sequence_sec[i+1])\n    except IndexError:\n        flag = (times_sec > sequence_sec[i] + t_training_delay)\n    labels[flag] = int(sequence[i][1])\n\nidx_last_flag = np.where(labels != 0)[0][-1]\n\nW_hd = np.random.binomial(n=1, p=0.05, size=(n_hd, n_dense))  #Test random sparse weights\nx_hd = x_dense @ W_hd.T\nranks = np.argsort(np.argsort(-x_hd, axis=1), axis=1)\nz_hd = np.where(ranks < k, 1., 0.)\nW_out = np.zeros((n_out, n_hd))\nW = np.zeros((n_out, n_hd))\n\nz_out_train = np.zeros((z_hd.shape[0],  n_out))\nfor i, row in enumerate(z_hd[:idx_last_flag]):\n    teacher = np.zeros((n_out,))\n    if labels[i] != 0:\n        teacher[int(labels[i]-1)] = w_teacher\n    out = row @ W_out.T + teacher\n    z_out_train[i] = out - teacher\n    dW = (1./n_pot)*(np.atleast_2d(out).T @ np.atleast_2d(row))\n    W += dW\n    W_out = np.where(W>=1., 1./k, 0.)\n    # if i%100 == 0:\n    #     print(np.sum(W_out, axis=1))\n\nz_out_acc = np.zeros((z_hd.shape[0],  n_out))\nfor i, row in enumerate(z_hd):\n    out = row @ W_out.T\n    z_out_acc[i] = out\n\nranks_out = np.argsort(np.argsort(-z_out_acc, axis=1), axis=1)\nz_wta = np.where(ranks_out < 1, 1., 0.)\n\nz_pred = np.zeros_like(sequence_sec)\nz_true = np.zeros_like(sequence_sec)\nfor i, t in enumerate(sequence_sec):\n    try:\n        flag = (times_sec > sequence_sec[i] + t_training_delay) & (times_sec < sequence_sec[i+1])\n    except IndexError:\n        flag = (times_sec > sequence_sec[i] + t_training_delay)\n    z_pred[i] = np.argsort(np.sum(z_out_acc[flag], axis=0))[-1] + 1\n    z_true[i] = sequence[i][1]\n\ntrain_acc = sklearn.metrics.accuracy_score(z_true[:n_train], z_pred[:n_train])\ntest_acc = sklearn.metrics.accuracy_score(z_true[n_train:], z_pred[n_train:])\n\nz_out = np.empty((z_hd.shape[0],  n_out))\nz_out[:idx_last_flag] = z_out_train[:idx_last_flag]\nz_out[idx_last_flag:] = z_out_acc[idx_last_flag:]\n\nprint(f'k: {k}, n_pot: {n_pot}, t_training_delay: {t_training_delay}')\nprint(f'Train accuracy: {train_acc:.4f}, Test accuracy: {test_acc:.4f}')",
   "id": "e12897cdc9d9840f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----- config -----\n",
    "top_intervals_idx = [(0, 10), (215, 225)]   # left & middle (sequence-index windows)\n",
    "j0, j1 = 235, 245                            # right \"Test\" (sequence-index)\n",
    "max_len = 20\n",
    "sigma = 2.\n",
    "savepath = 'figs/hd_out'\n",
    "\n",
    "# connector + slash styling\n",
    "connector_color   = 'black'\n",
    "connector_lw      = 1.5\n",
    "connector_ls      = ':'      # dotted\n",
    "slash_color       = 'black'\n",
    "slash_lw          = 1.5\n",
    "slash_len_fig     = 0.015\n",
    "slash_angle_deg   = 80\n",
    "slash_inset       = 0.003\n",
    "# right slashes are NOT mirrored\n",
    "\n",
    "# ----- style -----\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"legend.fontsize\": 10\n",
    "})\n",
    "cm = plt.get_cmap('tab10')\n",
    "\n",
    "# ----- per-sample block labels over full recording -----\n",
    "colour = np.zeros_like(times_sec, dtype=int)\n",
    "for i in range(len(sequence_sec)):\n",
    "    t_start = sequence_sec[i]\n",
    "    t_end = sequence_sec[i + 1] if i + 1 < len(sequence_sec) else np.inf\n",
    "    mask = (times_sec >= t_start) & (times_sec < t_end)\n",
    "    colour[mask] = int(sequence[i][1])\n",
    "\n",
    "# ----- helpers -----\n",
    "def seq_idx_window_to_sample_idx(a_idx, b_idx):\n",
    "    a_idx = int(np.clip(a_idx, 0, len(sequence_sec) - 1))\n",
    "    b_idx = int(np.clip(b_idx, 0, len(sequence_sec) - 1))\n",
    "    t0_sec = sequence_sec[a_idx]\n",
    "    t1_sec = sequence_sec[b_idx]\n",
    "    t0 = int(np.abs(times_sec - t0_sec).argmin())\n",
    "    t1 = int(np.abs(times_sec - t1_sec).argmin())\n",
    "    if t1 <= t0:\n",
    "        t1 = min(t0 + 1, len(times_sec))\n",
    "    return t0, t1\n",
    "\n",
    "def find_blocks(labels, max_len=20, ignore=0):\n",
    "    labels = np.asarray(labels)\n",
    "    n = len(labels)\n",
    "    if n == 0: return []\n",
    "    blocks, start, prev = [], 0, labels[0]\n",
    "    for i in range(1, n + 1):\n",
    "        cur = labels[i] if i < n else None\n",
    "        if cur != prev:\n",
    "            if prev != ignore:\n",
    "                run_start, run_end, lab = start, i, int(prev)\n",
    "                s = run_start\n",
    "                while s < run_end:\n",
    "                    e = min(s + max_len, run_end)\n",
    "                    blocks.append((s, e, lab))  # [s, e) end-exclusive\n",
    "                    s = e\n",
    "            start, prev = i, cur\n",
    "    return blocks\n",
    "\n",
    "# Use midpoints between samples as span edges -> no visual gaps\n",
    "def block_edges_from_indices(x, s, e):\n",
    "    if s <= 0:\n",
    "        L = x[0]\n",
    "    else:\n",
    "        L = 0.5 * (x[s-1] + x[s])\n",
    "    if e >= len(x):\n",
    "        R = x[-1]\n",
    "    else:\n",
    "        R = 0.5 * (x[e-1] + x[e])\n",
    "    return L, R\n",
    "\n",
    "# ----- figure (1x3) -----\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 1.5),\n",
    "                       sharey=False,\n",
    "                       gridspec_kw={'wspace': 0.25})\n",
    "ax_l, ax_m, ax_r = ax[0], ax[1], ax[2]\n",
    "\n",
    "# windows (sample indices)\n",
    "t0_a, t1_a       = seq_idx_window_to_sample_idx(*top_intervals_idx[0])  # left\n",
    "t0_b, t1_b       = seq_idx_window_to_sample_idx(*top_intervals_idx[1])  # middle\n",
    "t0_test, t1_test = seq_idx_window_to_sample_idx(j0, j1)                 # right\n",
    "\n",
    "# GLOBAL origin: first time from LEFT window\n",
    "x0_ref = times_sec[t0_a]\n",
    "\n",
    "def plot_interval_time_x(ah, t0, t1, title=None, hide_y=False):\n",
    "    # time axis shifted by the SAME origin (x0_ref)\n",
    "    x_raw = times_sec[t0:t1]\n",
    "    x = x_raw - x0_ref\n",
    "\n",
    "    labels_local = colour[t0:t1]\n",
    "    blocks = find_blocks(labels_local, max_len=max_len)\n",
    "\n",
    "    # traces vs time\n",
    "    for i in range(z_out.shape[1]):\n",
    "        smoothed = gaussian_filter1d(z_out[t0:t1, i], sigma=sigma)\n",
    "        ah.plot(x, smoothed, label=f'Neuron {i+1}', color=cm(i % cm.N), lw=1.5, alpha=0.85)\n",
    "\n",
    "    # shaded blocks (midpoint edges => no gaps)\n",
    "    for s, e, lab in blocks:\n",
    "        if lab == 0:\n",
    "            continue\n",
    "        L, R = block_edges_from_indices(x, s, e)\n",
    "        if R > L:\n",
    "            ah.axvspan(L, R, facecolor=cm((lab-1) % cm.N), alpha=0.15, linewidth=0)\n",
    "\n",
    "    # axes styling\n",
    "    for side in ['top','right','left','bottom']:\n",
    "        ah.spines[side].set_visible(side in ['left','bottom'])\n",
    "        ah.spines[side].set_linewidth(1.5)\n",
    "    ah.tick_params(axis='both', which='both', labelsize=12, width=1.5)\n",
    "    ah.set_xlim(x[0], x[-1])\n",
    "    ah.set_ylim(0., 1.)\n",
    "\n",
    "    if not hide_y:\n",
    "        ah.set_yticks([0, 1])\n",
    "    else:\n",
    "        ah.set_yticks([])\n",
    "        ah.set_ylabel(\"\")\n",
    "        ah.spines['left'].set_visible(False)\n",
    "\n",
    "    if title:\n",
    "        ah.text(0.99, 1.06, title, transform=ah.transAxes,\n",
    "                va=\"center\", ha=\"right\", fontsize=14)\n",
    "\n",
    "# ----- plot panels (single row) -----\n",
    "plot_interval_time_x(ax_l, t0_a,   t1_a,   title=None,   hide_y=False)  # left\n",
    "plot_interval_time_x(ax_m, t0_b,   t1_b,   title=\"Train\", hide_y=True)  # middle (hide y)\n",
    "plot_interval_time_x(ax_r, t0_test,t1_test,title=\"Test\",  hide_y=True) # right\n",
    "\n",
    "# labels\n",
    "ax_l.set_ylabel(\"Activity (a.u.)\", fontsize=12)\n",
    "ax_m.set_xlabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "# legend (from left), outside on the right\n",
    "handles, labels = ax_l.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, frameon=False, loc=\"center left\",\n",
    "           bbox_to_anchor=(0.92, 0.80), ncol=1)\n",
    "\n",
    "# ----- black dotted connectors + slashes (centered on axis; equal above/below) -----\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# helper: bottom-spine anchor at x∈[0,1] in *axes* coords → figure coords\n",
    "def bottom_anchor_in_fig(ax, x_in_axes):\n",
    "    pt_disp = ax.transAxes.transform((x_in_axes, 0.0))            # axes → display\n",
    "    return fig.transFigure.inverted().transform(pt_disp)          # display → figure\n",
    "\n",
    "# exact anchors at spine ends\n",
    "pL_right = bottom_anchor_in_fig(ax_l, 1.0)\n",
    "pM_left  = bottom_anchor_in_fig(ax_m, 0.0)\n",
    "pM_right = bottom_anchor_in_fig(ax_m, 1.0)\n",
    "pR_left  = bottom_anchor_in_fig(ax_r, 0.0)\n",
    "pR_right = bottom_anchor_in_fig(ax_r, 1.0)\n",
    "\n",
    "# geometry (identical everywhere)\n",
    "theta  = np.deg2rad(slash_angle_deg)\n",
    "dx_fig = slash_len_fig * np.cos(theta)  # total horizontal span of the slash\n",
    "dy_fig = slash_len_fig * np.sin(theta)  # total vertical span of the slash\n",
    "stub   = 0.04                           # dotted continuation length (figure coords)\n",
    "\n",
    "def draw_centered_slash(anchor_xy):\n",
    "    \"\"\"Draw a slash centered on the anchor, spanning equally above/below the axis.\"\"\"\n",
    "    xA, yA = anchor_xy\n",
    "    fig.add_artist(Line2D([xA - dx_fig/2, xA + dx_fig/2],\n",
    "                          [yA - dy_fig/2, yA + dy_fig/2],\n",
    "                          transform=fig.transFigure, lw=slash_lw,\n",
    "                          color=slash_color, clip_on=False))\n",
    "\n",
    "def draw_gap(p_left, p_right):\n",
    "    \"\"\"Two centered slashes at both anchors + dotted connector between their endpoints.\"\"\"\n",
    "    xL, yL = p_left\n",
    "    xR, yR = p_right\n",
    "    # slashes (centered, same orientation)\n",
    "    draw_centered_slash((xL, yL))\n",
    "    draw_centered_slash((xR, yR))\n",
    "    # dotted connector from right end of left slash to left end of right slash (baseline = left y)\n",
    "    fig.add_artist(Line2D([xL + dx_fig/2, xR - dx_fig/2], [yL, yL],\n",
    "                          transform=fig.transFigure, linestyle=connector_ls,\n",
    "                          lw=connector_lw, color=connector_color, clip_on=False))\n",
    "\n",
    "# Left ↔ Middle\n",
    "draw_gap(pL_right, pM_left)\n",
    "\n",
    "# Middle ↔ Right\n",
    "draw_gap(pM_right, pR_left)\n",
    "\n",
    "# Right-edge: centered slash at the exact right end + dotted continuation\n",
    "xE, yE = pR_right\n",
    "draw_centered_slash((xE, yE))\n",
    "fig.add_artist(Line2D([xE + dx_fig/2, xE + dx_fig/2 + stub], [yE, yE],\n",
    "                      transform=fig.transFigure, linestyle=connector_ls,\n",
    "                      lw=connector_lw, color=connector_color, clip_on=False))\n",
    "\n",
    "\n",
    "plt.savefig(f'{savepath}.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{savepath}.png', dpi=200, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "4689f5a390036333",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1,4, figsize = (10,4))\n",
    "\n",
    "ax[0].imshow(np.cov(sensor_data.T))\n",
    "ax[1].imshow(np.cov(adap_whitening(sensor_data)[0].T))\n",
    "ax[2].imshow(np.cov(adap_whitening_2(sensor_data)[0].T))\n",
    "ax[3].imshow(np.cov(whiten(sensor_data).T))\n",
    "\n",
    "for a in fig.axes:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "    a.spines[:].set_visible(False)"
   ],
   "id": "fc82b11737cd2ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "adap_whitening_2(sensor_data)",
   "id": "11edc5c4786abc9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6c576a0ae86730f5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
